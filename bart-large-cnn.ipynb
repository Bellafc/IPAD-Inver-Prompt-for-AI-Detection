{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d76228-a1a6-462e-abf2-366f941505c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: modelscope in ./miniconda3/lib/python3.12/site-packages (1.22.3)\n",
      "Requirement already satisfied: requests>=2.25 in ./miniconda3/lib/python3.12/site-packages (from modelscope) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./miniconda3/lib/python3.12/site-packages (from modelscope) (4.66.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in ./miniconda3/lib/python3.12/site-packages (from modelscope) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading Model to directory: /root/.cache/modelscope/hub/Xenova/bart-large-cnn\n"
     ]
    }
   ],
   "source": [
    "!pip install modelscope\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('Xenova/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd45abc-46bf-4701-8ab3-493766dd5639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3933444023132324]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 运行测试代码\u001b[39;00m\n\u001b[1;32m     95\u001b[0m scorer \u001b[38;5;241m=\u001b[39m BARTScorer(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 如果没有 GPU，可改为 \"cpu\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 91\u001b[0m, in \u001b[0;36mBARTScorer.test\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(src_list, tgt_list, batch_size)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mhum\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mscore\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hum' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BARTScorer:\n",
    "    def __init__(self, device='cuda:0', max_length=1024, model_dir=\".cache/modelscope/hub/Xenova/bart-large-cnn\"):\n",
    "        \"\"\" \n",
    "        device: 运行设备 (cuda:0 / cpu)\n",
    "        max_length: 最大序列长度\n",
    "        model_dir: 本地模型路径，默认使用 modelscope 下载的路径\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 如果没有提供 model_dir，则先下载模型\n",
    "        # if model_dir is None:\n",
    "        #     model_dir = snapshot_download('Xenova/bart-large-cnn')\n",
    "\n",
    "        # 从本地加载 tokenizer 和 model\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # 设置损失函数\n",
    "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
    "        self.lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def score(self, srcs, tgts, batch_size=16):\n",
    "        \"\"\" 计算文本相似性得分 \"\"\"\n",
    "        score_list = []\n",
    "        for i in range(0, len(srcs), batch_size):\n",
    "            src_list = srcs[i: i + batch_size]\n",
    "            tgt_list = tgts[i: i + batch_size]\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    encoded_src = self.tokenizer(\n",
    "                        src_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    encoded_tgt = self.tokenizer(\n",
    "                        tgt_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    src_tokens = encoded_src['input_ids'].to(self.device)\n",
    "                    src_mask = encoded_src['attention_mask'].to(self.device)\n",
    "\n",
    "                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)\n",
    "                    tgt_mask = encoded_tgt['attention_mask']\n",
    "                    tgt_len = tgt_mask.sum(dim=1).to(self.device)\n",
    "\n",
    "                    output = self.model(\n",
    "                        input_ids=src_tokens,\n",
    "                        attention_mask=src_mask,\n",
    "                        labels=tgt_tokens\n",
    "                    )\n",
    "                    logits = output.logits.view(-1, self.model.config.vocab_size)\n",
    "                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))\n",
    "                    loss = loss.view(tgt_tokens.shape[0], -1)\n",
    "                    loss = loss.sum(dim=1) / tgt_len\n",
    "                    curr_score_list = [-x.item() for x in loss]\n",
    "                    score_list += curr_score_list\n",
    "\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                print(f'source: {src_list}')\n",
    "                print(f'target: {tgt_list}')\n",
    "                exit(0)\n",
    "        return score_list\n",
    "\n",
    "    def test(self, batch_size=1):\n",
    "        \"\"\" 测试方法 \"\"\"\n",
    "        src_list = [\"Within the past decade, humans have watched in awe as technology has improved exponentially, and learned to take on tasks once thought only humans could accomplish. Technology has been taught to communicate, entertain, solve problems, along with many other functions, and now it may be able to teach as well. Though we have made use of every technological advancement in the past, this may be one that we should not capitalize on. Before taking this enormous step in technological advancement, humans must consider the flaws within this newly developed system, whether we are ready to replace teachers with computers. Though this new system is an astonishing feat in technology, people must recognize the flaws within this system as well. As described in paragraph 6, a student's facial expression has the power to change a lesson. If this is true, could a student not simply fake an emotion to get out of assignments or slack off by acting confused. The article claims that the software can pick up on fake smiles, but can it detect false expressions for other emotions as well? Another flaw within this system is the emotions the technology is capable of perceiving, paragraph 6 discusses the modification of a lesson based on boredom or confusion detected by the computer, however, neither of these two feelings are listed in the 6 emotions that the software is capable of detecting (paragraph 3). These two major flaws within the software lead to only one conclusion, the technology is simply not advanced enough to take on the role of a teacher. Humans must also consider this issue from an ethical standpoint, are we ready to take away the role of teachers and replace them with software? Though machines have already taken away menial jobs, such as factory work, teaching is a career which requires a four year college degree and a license. Countless people aspire to become teachers, are we ready to take that away? Even if teachers were to work hand in hand with this technology there would still be\"]\n",
    "\n",
    "        tgt_list = ['Discuss the potential flaws and ethical dilemmas associated with the integration of emotion detecting software in the classroom, and argue whether or not humans are ready to replace teachers with computers.']\n",
    "        \n",
    "        score=self.score(src_list, tgt_list, batch_size)\n",
    "        print(score)\n",
    "        hum['prompt score']=score\n",
    "\n",
    "\n",
    "# 运行测试代码\n",
    "scorer = BARTScorer(device=\"cuda:0\")  # 如果没有 GPU，可改为 \"cpu\"\n",
    "scorer.test(batch_size=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
